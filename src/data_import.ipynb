{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import tzlocal\n",
    "import pytz\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    db_host='localhost'\n",
    "    db_port=8086\n",
    "    db_name='meteorology'\n",
    "    stations = ['mythenquai', 'tiefenbrunnen']\n",
    "    stations_force_query_last_entry = False\n",
    "    stations_last_entries = {}\n",
    "    keys_mapping = {\n",
    "        \"values.air_temperature.value\": \"air_temperature\",\n",
    "        \"values.barometric_pressure_qfe.value\": \"barometric_pressure_qfe\",\n",
    "        \"values.dew_point.value\": \"dew_point\",\n",
    "        \"values.global_radiation.value\": \"global_radiation\",\n",
    "        \"values.humidity.value\": \"humidity\",\n",
    "        \"values.precipitation.value\": \"precipitation\",\n",
    "        \"values.timestamp_cet.value\": \"timestamp_cet\",\n",
    "        \"values.water_temperature.value\": \"water_temperature\",\n",
    "        \"values.wind_direction.value\": \"wind_direction\",\n",
    "        \"values.wind_force_avg_10min.value\": \"wind_force_avg_10min\",\n",
    "        \"values.wind_gust_max_10min.value\": \"wind_gust_max_10min\",\n",
    "        \"values.wind_speed_avg_10min.value\": \"wind_speed_avg_10min\",\n",
    "        \"values.windchill.value\": \"windchill\"\n",
    "    }\n",
    "    historic_data_folder = '.'+os.sep+'data'\n",
    "    historic_data_chunksize = 10000\n",
    "    historic_data_sleep_sec = 0\n",
    "    client = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __set_last_db_entry(config, station, entry):\n",
    "    current_last_time = __extract_last_db_day(config.stations_last_entries.get(station, None), station, None)\n",
    "    entry_time = __extract_last_db_day(entry, station, None)\n",
    "\n",
    "    if current_last_time is None and entry_time is not None:\n",
    "        config.stations_last_entries[station] = entry\n",
    "    elif current_last_time is not None and entry_time is not None and current_last_time < entry_time:\n",
    "        config.stations_last_entries[station] = entry\n",
    "\n",
    "def __get_last_db_entry(config, station):\n",
    "    last_entry = None\n",
    "    if not config.stations_force_query_last_entry:\n",
    "        # speedup for Raspberry Pi - last entry query takes > 2 Sec.!\n",
    "        last_entry = config.stations_last_entries.get(station, None)\n",
    "    \n",
    "    if last_entry is None:\n",
    "        try:\n",
    "            # we are only interested in time, however need to provide any field to make query work\n",
    "            query = \"SELECT last(air_temperature) FROM \\\"{}\\\"\".format(station)\n",
    "            last_entry = config.client.query(query)\n",
    "        except:\n",
    "            # There are influxDB versions which have an issue with above \"last\" query\n",
    "            print(\"An exception occurred while querying last entry from DB for \"+ station +\". Try alternative approach.\") \n",
    "            query = \"SELECT * FROM \\\"{}\\\" ORDER BY time DESC LIMIT 1\".format(station)\n",
    "            last_entry = config.client.query(query) \n",
    "        \n",
    "    __set_last_db_entry(config, station, last_entry)\n",
    "    return last_entry\n",
    "    \n",
    "def __extract_last_db_day(last_entry, station, default_last_db_day):\n",
    "    if last_entry is not None:\n",
    "        val = None\n",
    "        if isinstance(last_entry, pd.DataFrame):\n",
    "            val = last_entry\n",
    "        elif isinstance(last_entry, dict):\n",
    "            val = last_entry.get(station, None)   \n",
    "            \n",
    "        if val is not None: \n",
    "            if not val.index.empty:\n",
    "                return val.index[0]\n",
    "        \n",
    "    return default_last_db_day\n",
    "\n",
    "def __get_data_of_day(day, station):\n",
    "    # convert to local time of station\n",
    "    day = day.tz_convert('Europe/Zurich')\n",
    "    base_url = 'https://tecdottir.herokuapp.com/measurements/{}'\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(\"Query \"+ station +\" at \"+day_str)\n",
    "    payload = {'startDate': day_str, 'endDate': day_str}\n",
    "    url = base_url.format(station)\n",
    "    response = requests.get(url, params=payload)\n",
    "    if(response.ok):\n",
    "        #print(response.json())\n",
    "        jData = json.loads(response.content)\n",
    "        return jData\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "def __define_types(data, date_format):\n",
    "    data['timestamp_cet'] = pd.to_datetime(data['timestamp_cet'], format=date_format)\n",
    "    if not data.empty and data['timestamp_cet'].iloc[0].tzinfo is None:\n",
    "        data['timestamp_cet'] = data['timestamp_cet'].dt.tz_localize('Europe/Zurich', ambiguous=True).dt.tz_convert('UTC')\n",
    "    data.set_index('timestamp_cet', inplace=True)\n",
    "    \n",
    "    for column in data.columns[0:]:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def __clean_data(config, data_of_last_day, last_db_entry, date_format, station):\n",
    "    normalized = json_normalize(data_of_last_day['result'])\n",
    "    \n",
    "    for column in normalized.columns[0:]:   \n",
    "        mapping = config.keys_mapping.get(column, None)\n",
    "        if mapping is not None:\n",
    "            normalized[mapping] = normalized[column]\n",
    "            \n",
    "        normalized.drop(columns=column, inplace=True)\n",
    "    \n",
    "    # make sure types/index are correct\n",
    "    normalized = __define_types(normalized, date_format)\n",
    "    \n",
    "    #print(\"Normalized index \"+str(normalized.index))\n",
    "    #print(\"Last db index \"+str(lastDBEntry[station].index))\n",
    "    \n",
    "    # remove all entries older than last element\n",
    "    last_db_entry_time = None\n",
    "    if isinstance(last_db_entry, pd.DataFrame):\n",
    "        last_db_entry_time = last_db_entry\n",
    "    elif isinstance(last_db_entry, dict):\n",
    "        last_db_entry_time = last_db_entry.get(station, None) \n",
    "    last_db_entry_time = last_db_entry_time.index[0] #.replace(tzinfo=None)\n",
    "    # print(\"Last \"+str(last_db_entry_time) +\" elements \"+str(normalized.index[0]) +\" - \"+str(normalized.index[-1]))\n",
    "    normalized.drop(normalized[normalized.index <= last_db_entry_time].index, inplace=True)\n",
    "    \n",
    "    return normalized\n",
    "        \n",
    "def __add_data_to_db(config, data, station):\n",
    "    config.client.write_points(data, station, time_precision='s', database=config.db_name)\n",
    "    __set_last_db_entry(config, station, data.tail(1))\n",
    "    \n",
    "def __append_df_to_csv(data, csv_file_path, sep=\",\"):\n",
    "    header = False\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        header = True\n",
    "\n",
    "    data.to_csv(csv_file_path, mode='a', sep=sep, header=header)\n",
    "    \n",
    "def __signal_handler(sig, frame):\n",
    "    sys.exit(0)\n",
    "    \n",
    "def connect_db(config):\n",
    "    \"\"\"Connects to the database and initializes the client\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info\n",
    "\n",
    "   \"\"\"\n",
    "    if config.client is None:\n",
    "        # https://www.influxdata.com/blog/getting-started-python-influxdb/\n",
    "        config.client = DataFrameClient(host=config.db_host, port=config.db_port, database=config.db_name)\n",
    "        config.client.switch_database(config.db_name)\n",
    "\n",
    "def clean_db(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    config.client.drop_database(config.db_name)\n",
    "    config.client.create_database(config.db_name)\n",
    "    config.stations_last_entries.clear()\n",
    "\n",
    "def import_historic_data(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    # read historic data from files\n",
    "    \n",
    "    for station in config.stations:\n",
    "        last_entry = __get_last_db_entry(config, station)\n",
    "          \n",
    "        if last_entry is None or not last_entry:\n",
    "            print(\"Load historic data for \"+station + \" ...\")\n",
    "        \n",
    "            file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_2007-2018.csv\")\n",
    "            if os.path.isfile(file_name):\n",
    "                print(\"\\tLoad \"+ file_name)\n",
    "                for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                    chunk = __define_types(chunk, '%Y-%m-%dT%H:%M:%S')\n",
    "                    print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                    __add_data_to_db(config, chunk, station)\n",
    "                    \n",
    "                    if config.historic_data_sleep_sec > 0:\n",
    "                        sleep(config.historic_data_sleep_sec)\n",
    "            else:\n",
    "                print(file_name +\" does not seem to exist.\")\n",
    "                \n",
    "            current_time = datetime.datetime.now(tzlocal.get_localzone())\n",
    "            running_year = 2019\n",
    "            while running_year <= current_time.year:\n",
    "                file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(running_year) +\".csv\")\n",
    "                if os.path.isfile(file_name):\n",
    "                    print(\"\\tLoad \"+ file_name)\n",
    "                    for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                        chunk = __define_types(chunk, '%Y-%m-%d %H:%M:%S')\n",
    "                        print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                        __add_data_to_db(config, chunk, station)\n",
    "                        \n",
    "                        if config.historic_data_sleep_sec > 0:\n",
    "                            sleep(config.historic_data_sleep_sec)\n",
    "                else:\n",
    "                    print(file_name +\" does not seem to exist.\")\n",
    "                running_year+=1\n",
    "        else:\n",
    "            print(\"There is already data for \"+station + \". No historic data will be imported.\")\n",
    "    \n",
    "        print(\"Historic data for \"+station+\" loaded.\")\n",
    "        \n",
    "        \n",
    "def import_latest_data(config, append_to_csv=False, periodic_read=False):\n",
    "    \"\"\"Reads the latest data from the Wasserschutzpolizei Zurich weather API\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "    append_to_csv (bool): Defines if the data should be appended to a CSV file\n",
    "    periodic_read (bool): Defines if the function should keep reading after it imported the latest data (blocking through a sleep)\n",
    "\n",
    "   \"\"\"\n",
    "    # access API for current data\n",
    "    current_time = datetime.datetime.now(pytz.utc)\n",
    "    current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    last_db_days = [current_day] * len(config.stations)\n",
    "    new_data_received = True\n",
    "\n",
    "    for idx, station in enumerate(config.stations):\n",
    "        last_db_entry = __get_last_db_entry(config, station)\n",
    "        last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "\n",
    "    if periodic_read:\n",
    "        signal.signal(signal.SIGINT, __signal_handler)\n",
    "        print(\"\\nPress Ctrl+C to stop!\\n\")\n",
    "\n",
    "    while True:\n",
    "        current_time = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "        # check if all historic data (retrieved from API) has been processed \n",
    "        last_db_day = max(last_db_days)\n",
    "        if periodic_read and last_db_day >= current_day: \n",
    "            # once every 10 Min\n",
    "            sleep_until = current_time + datetime.timedelta(minutes=10)       \n",
    "            # once per day\n",
    "            # sleep_until = current_time + datetime.timedelta(days=1)\n",
    "            # sleep_until = sleep_until.replace(hour=6, minute=0, second=0, microsecond=0)\n",
    "            sleep_sec = (sleep_until - current_time).total_seconds()\n",
    "\n",
    "            print(\"Sleep for \"+str(sleep_sec) + \"s (from \" + str(current_time) +\" until \"+str(sleep_until) + \") when next data will be queried.\")\n",
    "            sleep(sleep_sec)\n",
    "            current_time = datetime.datetime.now(pytz.utc)\n",
    "            current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        elif not periodic_read and not new_data_received:\n",
    "            # stop here\n",
    "            return;\n",
    "\n",
    "        new_data_received = False\n",
    "        for idx, station in enumerate(config.stations):\n",
    "            last_db_entry = __get_last_db_entry(config, station)\n",
    "            last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "            data_of_last_db_day = __get_data_of_day(last_db_days[idx], station)\n",
    "\n",
    "            normalized_data = __clean_data(config, data_of_last_db_day, last_db_entry, '%d.%m.%Y %H:%M:%S', station)\n",
    "\n",
    "            if normalized_data.size > 0:\n",
    "                new_data_received = True\n",
    "                __add_data_to_db(config, normalized_data, station)\n",
    "                if append_to_csv:\n",
    "                    __append_df_to_csv(normalized_data, os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(current_time.year) +\".csv\"))\n",
    "                print(\"Handle \"+ station +\" from \"+ str(normalized_data.index[0]) +\" to \"+ str(normalized_data.index[-1])) \n",
    "            else:\n",
    "                print(\"No new data received for \"+station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load historic data for mythenquai ...\n",
      "\tLoad ../data/messwerte_mythenquai_2007-2018.csv\n",
      "Add mythenquai from 2007-04-22 19:20:00+00:00 to 2007-07-01 05:50:00+00:00\n",
      "Add mythenquai from 2007-07-01 06:00:00+00:00 to 2007-09-08 16:30:00+00:00\n",
      "Add mythenquai from 2007-09-08 16:40:00+00:00 to 2007-11-17 04:10:00+00:00\n",
      "Add mythenquai from 2007-11-17 04:20:00+00:00 to 2008-01-25 14:50:00+00:00\n",
      "Add mythenquai from 2008-01-25 15:00:00+00:00 to 2008-04-04 01:40:00+00:00\n",
      "Add mythenquai from 2008-04-04 01:50:00+00:00 to 2008-06-12 13:50:00+00:00\n",
      "Add mythenquai from 2008-06-12 14:00:00+00:00 to 2008-08-21 02:20:00+00:00\n",
      "Add mythenquai from 2008-08-21 02:30:00+00:00 to 2008-10-29 16:30:00+00:00\n",
      "Add mythenquai from 2008-10-29 16:40:00+00:00 to 2009-01-07 13:40:00+00:00\n",
      "Add mythenquai from 2009-01-07 13:50:00+00:00 to 2009-03-18 17:20:00+00:00\n",
      "Add mythenquai from 2009-03-18 17:30:00+00:00 to 2009-05-27 20:50:00+00:00\n",
      "Add mythenquai from 2009-05-27 21:00:00+00:00 to 2009-08-05 12:00:00+00:00\n",
      "Add mythenquai from 2009-08-05 12:10:00+00:00 to 2009-10-13 22:50:00+00:00\n",
      "Add mythenquai from 2009-10-13 23:00:00+00:00 to 2009-12-22 10:50:00+00:00\n",
      "Add mythenquai from 2009-12-22 11:00:00+00:00 to 2010-03-01 21:30:00+00:00\n",
      "Add mythenquai from 2010-03-01 21:40:00+00:00 to 2010-05-10 08:20:00+00:00\n",
      "Add mythenquai from 2010-05-10 08:30:00+00:00 to 2010-07-18 21:10:00+00:00\n",
      "Add mythenquai from 2010-07-18 21:20:00+00:00 to 2010-09-26 08:10:00+00:00\n",
      "Add mythenquai from 2010-09-26 08:20:00+00:00 to 2011-01-23 00:10:00+00:00\n",
      "Add mythenquai from 2011-01-23 00:20:00+00:00 to 2011-04-02 14:40:00+00:00\n",
      "Add mythenquai from 2011-04-02 14:50:00+00:00 to 2011-06-11 01:20:00+00:00\n",
      "Add mythenquai from 2011-06-11 01:30:00+00:00 to 2011-08-20 14:30:00+00:00\n",
      "Add mythenquai from 2011-08-20 14:40:00+00:00 to 2011-10-29 01:40:00+00:00\n",
      "Add mythenquai from 2011-10-29 01:50:00+00:00 to 2012-01-06 17:20:00+00:00\n",
      "Add mythenquai from 2012-01-06 17:30:00+00:00 to 2012-03-16 09:10:00+00:00\n",
      "Add mythenquai from 2012-03-16 09:20:00+00:00 to 2012-05-24 21:10:00+00:00\n",
      "Add mythenquai from 2012-05-24 21:20:00+00:00 to 2012-08-02 08:00:00+00:00\n",
      "Add mythenquai from 2012-08-02 08:10:00+00:00 to 2012-10-10 20:30:00+00:00\n",
      "Add mythenquai from 2012-10-10 20:40:00+00:00 to 2012-12-19 09:00:00+00:00\n",
      "Add mythenquai from 2012-12-19 09:10:00+00:00 to 2013-02-26 19:40:00+00:00\n",
      "Add mythenquai from 2013-02-26 19:50:00+00:00 to 2013-05-07 06:40:00+00:00\n",
      "Add mythenquai from 2013-05-07 06:50:00+00:00 to 2013-07-15 18:30:00+00:00\n",
      "Add mythenquai from 2013-07-15 18:40:00+00:00 to 2013-09-23 12:30:00+00:00\n",
      "Add mythenquai from 2013-09-23 12:40:00+00:00 to 2013-12-02 00:30:00+00:00\n",
      "Add mythenquai from 2013-12-02 00:40:00+00:00 to 2014-02-09 21:50:00+00:00\n",
      "Add mythenquai from 2014-02-09 22:00:00+00:00 to 2014-04-20 10:30:00+00:00\n",
      "Add mythenquai from 2014-04-20 10:40:00+00:00 to 2014-06-28 21:10:00+00:00\n",
      "Add mythenquai from 2014-06-28 21:20:00+00:00 to 2014-09-06 07:50:00+00:00\n",
      "Add mythenquai from 2014-09-06 08:00:00+00:00 to 2014-11-14 23:00:00+00:00\n",
      "Add mythenquai from 2014-11-14 23:10:00+00:00 to 2015-01-23 09:40:00+00:00\n",
      "Add mythenquai from 2015-01-23 09:50:00+00:00 to 2015-04-02 21:50:00+00:00\n",
      "Add mythenquai from 2015-04-02 22:00:00+00:00 to 2015-06-11 09:50:00+00:00\n",
      "Add mythenquai from 2015-06-11 10:00:00+00:00 to 2015-08-19 22:10:00+00:00\n",
      "Add mythenquai from 2015-08-19 22:20:00+00:00 to 2015-10-28 11:50:00+00:00\n",
      "Add mythenquai from 2015-10-28 12:00:00+00:00 to 2016-01-05 23:20:00+00:00\n",
      "Add mythenquai from 2016-01-05 23:30:00+00:00 to 2016-03-15 10:50:00+00:00\n",
      "Add mythenquai from 2016-03-15 11:00:00+00:00 to 2016-05-23 21:40:00+00:00\n",
      "Add mythenquai from 2016-05-23 21:50:00+00:00 to 2016-08-01 10:00:00+00:00\n",
      "Add mythenquai from 2016-08-01 10:10:00+00:00 to 2016-10-10 00:10:00+00:00\n",
      "Add mythenquai from 2016-10-10 00:20:00+00:00 to 2016-12-18 12:20:00+00:00\n",
      "Add mythenquai from 2016-12-18 12:30:00+00:00 to 2017-02-26 01:40:00+00:00\n",
      "Add mythenquai from 2017-02-26 01:50:00+00:00 to 2017-05-07 22:00:00+00:00\n",
      "Add mythenquai from 2017-05-07 22:10:00+00:00 to 2017-07-16 11:50:00+00:00\n",
      "Add mythenquai from 2017-07-16 12:00:00+00:00 to 2017-09-25 05:00:00+00:00\n",
      "Add mythenquai from 2017-09-25 05:10:00+00:00 to 2017-12-03 16:50:00+00:00\n",
      "Add mythenquai from 2017-12-03 17:00:00+00:00 to 2018-02-11 16:00:00+00:00\n",
      "Add mythenquai from 2018-02-11 16:10:00+00:00 to 2018-04-24 08:10:00+00:00\n",
      "Add mythenquai from 2018-04-24 08:20:00+00:00 to 2018-07-02 18:50:00+00:00\n",
      "Add mythenquai from 2018-07-02 19:00:00+00:00 to 2018-09-10 05:30:00+00:00\n",
      "Add mythenquai from 2018-09-10 05:40:00+00:00 to 2018-11-18 18:50:00+00:00\n",
      "Add mythenquai from 2018-11-18 19:00:00+00:00 to 2018-12-31 23:00:00+00:00\n",
      "\tLoad ../data/messwerte_mythenquai_2019.csv\n",
      "Add mythenquai from 2019-01-07 23:10:00+00:00 to 2019-03-18 09:50:00+00:00\n",
      "Add mythenquai from 2019-03-18 10:00:00+00:00 to 2019-05-26 20:30:00+00:00\n",
      "Add mythenquai from 2019-05-26 20:40:00+00:00 to 2019-07-04 10:00:00+00:00\n",
      "Historic data for mythenquai loaded.\n",
      "Load historic data for tiefenbrunnen ...\n",
      "\tLoad ../data/messwerte_tiefenbrunnen_2007-2018.csv\n",
      "Add tiefenbrunnen from 2007-04-15 09:30:00+00:00 to 2007-06-23 20:00:00+00:00\n",
      "Add tiefenbrunnen from 2007-06-23 20:10:00+00:00 to 2007-09-01 06:40:00+00:00\n",
      "Add tiefenbrunnen from 2007-09-01 06:50:00+00:00 to 2007-11-09 18:20:00+00:00\n",
      "Add tiefenbrunnen from 2007-11-09 18:30:00+00:00 to 2008-01-18 05:00:00+00:00\n",
      "Add tiefenbrunnen from 2008-01-18 05:10:00+00:00 to 2008-05-04 08:00:00+00:00\n",
      "Add tiefenbrunnen from 2008-05-04 08:10:00+00:00 to 2008-07-12 19:40:00+00:00\n",
      "Add tiefenbrunnen from 2008-07-12 19:50:00+00:00 to 2008-09-20 07:00:00+00:00\n",
      "Add tiefenbrunnen from 2008-09-20 07:10:00+00:00 to 2008-11-28 21:00:00+00:00\n",
      "Add tiefenbrunnen from 2008-11-28 21:10:00+00:00 to 2009-02-06 19:50:00+00:00\n",
      "Add tiefenbrunnen from 2009-02-06 20:00:00+00:00 to 2009-06-07 16:30:00+00:00\n",
      "Add tiefenbrunnen from 2009-06-07 16:40:00+00:00 to 2009-08-16 04:50:00+00:00\n",
      "Add tiefenbrunnen from 2009-08-16 05:00:00+00:00 to 2009-10-24 16:00:00+00:00\n",
      "Add tiefenbrunnen from 2009-10-24 16:10:00+00:00 to 2010-01-02 04:10:00+00:00\n",
      "Add tiefenbrunnen from 2010-01-02 04:20:00+00:00 to 2010-03-12 15:20:00+00:00\n",
      "Add tiefenbrunnen from 2010-03-12 15:30:00+00:00 to 2010-05-21 03:50:00+00:00\n",
      "Add tiefenbrunnen from 2010-05-21 04:00:00+00:00 to 2010-07-29 14:30:00+00:00\n",
      "Add tiefenbrunnen from 2010-07-29 14:40:00+00:00 to 2010-10-07 01:20:00+00:00\n",
      "Add tiefenbrunnen from 2010-10-07 01:30:00+00:00 to 2010-12-15 13:00:00+00:00\n",
      "Add tiefenbrunnen from 2010-12-15 13:10:00+00:00 to 2011-02-22 23:40:00+00:00\n",
      "Add tiefenbrunnen from 2011-02-22 23:50:00+00:00 to 2011-05-03 10:20:00+00:00\n",
      "Add tiefenbrunnen from 2011-05-03 10:30:00+00:00 to 2011-07-11 21:00:00+00:00\n",
      "Add tiefenbrunnen from 2011-07-11 21:10:00+00:00 to 2011-09-20 10:30:00+00:00\n",
      "Add tiefenbrunnen from 2011-09-20 10:40:00+00:00 to 2011-11-29 01:30:00+00:00\n",
      "Add tiefenbrunnen from 2011-11-29 01:40:00+00:00 to 2012-02-06 13:00:00+00:00\n",
      "Add tiefenbrunnen from 2012-02-06 13:10:00+00:00 to 2012-04-16 07:40:00+00:00\n",
      "Add tiefenbrunnen from 2012-04-16 07:50:00+00:00 to 2012-06-24 18:50:00+00:00\n",
      "Add tiefenbrunnen from 2012-06-24 19:00:00+00:00 to 2012-09-02 06:50:00+00:00\n",
      "Add tiefenbrunnen from 2012-09-02 07:00:00+00:00 to 2012-11-10 19:30:00+00:00\n",
      "Add tiefenbrunnen from 2012-11-10 19:40:00+00:00 to 2013-01-19 06:40:00+00:00\n",
      "Add tiefenbrunnen from 2013-01-19 06:50:00+00:00 to 2013-03-29 17:30:00+00:00\n",
      "Add tiefenbrunnen from 2013-03-29 17:40:00+00:00 to 2013-06-07 04:40:00+00:00\n",
      "Add tiefenbrunnen from 2013-06-07 04:50:00+00:00 to 2013-08-15 23:20:00+00:00\n",
      "Add tiefenbrunnen from 2013-08-15 23:30:00+00:00 to 2013-10-24 10:00:00+00:00\n",
      "Add tiefenbrunnen from 2013-10-24 10:10:00+00:00 to 2014-01-01 22:20:00+00:00\n",
      "Add tiefenbrunnen from 2014-01-01 22:30:00+00:00 to 2014-03-12 18:40:00+00:00\n",
      "Add tiefenbrunnen from 2014-03-12 18:50:00+00:00 to 2014-05-21 06:40:00+00:00\n",
      "Add tiefenbrunnen from 2014-05-21 06:50:00+00:00 to 2014-07-29 17:20:00+00:00\n",
      "Add tiefenbrunnen from 2014-07-29 17:30:00+00:00 to 2014-10-07 04:00:00+00:00\n",
      "Add tiefenbrunnen from 2014-10-07 04:10:00+00:00 to 2014-12-15 15:50:00+00:00\n",
      "Add tiefenbrunnen from 2014-12-15 16:00:00+00:00 to 2015-02-23 03:30:00+00:00\n",
      "Add tiefenbrunnen from 2015-02-23 03:40:00+00:00 to 2015-05-03 15:00:00+00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add tiefenbrunnen from 2015-05-03 15:10:00+00:00 to 2015-07-12 03:40:00+00:00\n",
      "Add tiefenbrunnen from 2015-07-12 03:50:00+00:00 to 2015-10-07 17:20:00+00:00\n",
      "Add tiefenbrunnen from 2015-10-07 17:30:00+00:00 to 2015-12-16 05:30:00+00:00\n",
      "Add tiefenbrunnen from 2015-12-16 05:40:00+00:00 to 2016-02-23 16:50:00+00:00\n",
      "Add tiefenbrunnen from 2016-02-23 17:00:00+00:00 to 2016-05-03 04:00:00+00:00\n",
      "Add tiefenbrunnen from 2016-05-03 04:10:00+00:00 to 2016-07-11 16:30:00+00:00\n",
      "Add tiefenbrunnen from 2016-07-11 16:40:00+00:00 to 2016-09-19 04:40:00+00:00\n",
      "Add tiefenbrunnen from 2016-09-19 04:50:00+00:00 to 2016-11-27 19:00:00+00:00\n",
      "Add tiefenbrunnen from 2016-11-27 19:10:00+00:00 to 2017-02-05 08:10:00+00:00\n",
      "Add tiefenbrunnen from 2017-02-05 08:20:00+00:00 to 2017-04-16 00:00:00+00:00\n",
      "Add tiefenbrunnen from 2017-04-16 00:10:00+00:00 to 2017-06-24 17:30:00+00:00\n",
      "Add tiefenbrunnen from 2017-06-24 17:40:00+00:00 to 2017-09-02 16:10:00+00:00\n",
      "Add tiefenbrunnen from 2017-09-02 16:20:00+00:00 to 2017-11-11 04:50:00+00:00\n",
      "Add tiefenbrunnen from 2017-11-11 05:00:00+00:00 to 2018-01-19 17:00:00+00:00\n",
      "Add tiefenbrunnen from 2018-01-19 17:10:00+00:00 to 2018-03-30 11:20:00+00:00\n",
      "Add tiefenbrunnen from 2018-03-30 11:30:00+00:00 to 2018-06-08 00:50:00+00:00\n",
      "Add tiefenbrunnen from 2018-06-08 01:00:00+00:00 to 2018-08-16 12:30:00+00:00\n",
      "Add tiefenbrunnen from 2018-08-16 12:40:00+00:00 to 2018-10-25 00:20:00+00:00\n",
      "Add tiefenbrunnen from 2018-10-25 00:30:00+00:00 to 2018-12-31 23:00:00+00:00\n",
      "\tLoad ../data/messwerte_tiefenbrunnen_2019.csv\n",
      "Add tiefenbrunnen from 2019-01-07 23:10:00+00:00 to 2019-03-18 11:00:00+00:00\n",
      "Add tiefenbrunnen from 2019-03-18 11:10:00+00:00 to 2019-05-26 21:50:00+00:00\n",
      "Add tiefenbrunnen from 2019-05-26 22:00:00+00:00 to 2019-07-04 10:00:00+00:00\n",
      "Historic data for tiefenbrunnen loaded.\n",
      "\n",
      "Press Ctrl+C to stop!\n",
      "\n",
      "Query mythenquai at 2019-07-04\n",
      "Handle mythenquai from 2019-07-04 10:10:00+00:00 to 2019-07-04 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-04\n",
      "Handle tiefenbrunnen from 2019-07-04 10:10:00+00:00 to 2019-07-04 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-05\n",
      "Handle mythenquai from 2019-07-04 22:10:00+00:00 to 2019-07-05 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-05\n",
      "Handle tiefenbrunnen from 2019-07-04 22:10:00+00:00 to 2019-07-05 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-06\n",
      "Handle mythenquai from 2019-07-05 22:10:00+00:00 to 2019-07-06 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-06\n",
      "Handle tiefenbrunnen from 2019-07-05 22:10:00+00:00 to 2019-07-06 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-07\n",
      "Handle mythenquai from 2019-07-06 22:10:00+00:00 to 2019-07-07 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-07\n",
      "Handle tiefenbrunnen from 2019-07-06 22:10:00+00:00 to 2019-07-07 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-08\n",
      "Handle mythenquai from 2019-07-07 22:10:00+00:00 to 2019-07-08 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-08\n",
      "Handle tiefenbrunnen from 2019-07-07 22:10:00+00:00 to 2019-07-08 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-09\n",
      "Handle mythenquai from 2019-07-08 22:10:00+00:00 to 2019-07-09 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-09\n",
      "Handle tiefenbrunnen from 2019-07-08 22:10:00+00:00 to 2019-07-09 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-10\n",
      "Handle mythenquai from 2019-07-09 22:10:00+00:00 to 2019-07-10 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-10\n",
      "Handle tiefenbrunnen from 2019-07-09 22:10:00+00:00 to 2019-07-10 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-11\n",
      "Handle mythenquai from 2019-07-10 22:10:00+00:00 to 2019-07-11 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-11\n",
      "Handle tiefenbrunnen from 2019-07-10 22:10:00+00:00 to 2019-07-11 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-12\n",
      "Handle mythenquai from 2019-07-11 22:10:00+00:00 to 2019-07-12 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-12\n",
      "Handle tiefenbrunnen from 2019-07-11 22:10:00+00:00 to 2019-07-12 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-13\n",
      "Handle mythenquai from 2019-07-12 22:10:00+00:00 to 2019-07-13 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-13\n",
      "Handle tiefenbrunnen from 2019-07-12 22:10:00+00:00 to 2019-07-13 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-14\n",
      "Handle mythenquai from 2019-07-13 22:10:00+00:00 to 2019-07-14 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-14\n",
      "Handle tiefenbrunnen from 2019-07-13 22:10:00+00:00 to 2019-07-14 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-15\n",
      "Handle mythenquai from 2019-07-14 22:10:00+00:00 to 2019-07-15 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-15\n",
      "Handle tiefenbrunnen from 2019-07-14 22:10:00+00:00 to 2019-07-15 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-16\n",
      "Handle mythenquai from 2019-07-15 22:10:00+00:00 to 2019-07-16 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-16\n",
      "Handle tiefenbrunnen from 2019-07-15 22:10:00+00:00 to 2019-07-16 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-17\n",
      "Handle mythenquai from 2019-07-16 22:10:00+00:00 to 2019-07-17 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-17\n",
      "Handle tiefenbrunnen from 2019-07-16 22:10:00+00:00 to 2019-07-17 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-18\n",
      "Handle mythenquai from 2019-07-17 22:10:00+00:00 to 2019-07-18 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-18\n",
      "Handle tiefenbrunnen from 2019-07-17 22:10:00+00:00 to 2019-07-18 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-19\n",
      "Handle mythenquai from 2019-07-18 22:10:00+00:00 to 2019-07-19 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-19\n",
      "Handle tiefenbrunnen from 2019-07-18 22:10:00+00:00 to 2019-07-19 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-20\n",
      "Handle mythenquai from 2019-07-19 22:10:00+00:00 to 2019-07-20 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-20\n",
      "Handle tiefenbrunnen from 2019-07-19 22:10:00+00:00 to 2019-07-20 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-21\n",
      "Handle mythenquai from 2019-07-20 22:10:00+00:00 to 2019-07-21 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-21\n",
      "Handle tiefenbrunnen from 2019-07-20 22:10:00+00:00 to 2019-07-21 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-22\n",
      "Handle mythenquai from 2019-07-21 22:10:00+00:00 to 2019-07-22 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-22\n",
      "Handle tiefenbrunnen from 2019-07-21 22:10:00+00:00 to 2019-07-22 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-23\n",
      "Handle mythenquai from 2019-07-22 22:10:00+00:00 to 2019-07-23 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-23\n",
      "Handle tiefenbrunnen from 2019-07-22 22:10:00+00:00 to 2019-07-23 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-24\n",
      "Handle mythenquai from 2019-07-23 22:10:00+00:00 to 2019-07-24 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-24\n",
      "Handle tiefenbrunnen from 2019-07-23 22:10:00+00:00 to 2019-07-24 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-25\n",
      "Handle mythenquai from 2019-07-24 22:10:00+00:00 to 2019-07-25 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-25\n",
      "Handle tiefenbrunnen from 2019-07-24 22:10:00+00:00 to 2019-07-25 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-26\n",
      "Handle mythenquai from 2019-07-25 22:10:00+00:00 to 2019-07-26 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-26\n",
      "Handle tiefenbrunnen from 2019-07-25 22:10:00+00:00 to 2019-07-26 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-27\n",
      "Handle mythenquai from 2019-07-26 22:10:00+00:00 to 2019-07-27 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-27\n",
      "Handle tiefenbrunnen from 2019-07-26 22:10:00+00:00 to 2019-07-27 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-28\n",
      "Handle mythenquai from 2019-07-27 22:10:00+00:00 to 2019-07-28 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-28\n",
      "Handle tiefenbrunnen from 2019-07-27 22:10:00+00:00 to 2019-07-28 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-29\n",
      "Handle mythenquai from 2019-07-28 22:10:00+00:00 to 2019-07-29 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-29\n",
      "Handle tiefenbrunnen from 2019-07-28 22:10:00+00:00 to 2019-07-29 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-30\n",
      "Handle mythenquai from 2019-07-29 22:10:00+00:00 to 2019-07-30 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-30\n",
      "Handle tiefenbrunnen from 2019-07-29 22:10:00+00:00 to 2019-07-30 22:00:00+00:00\n",
      "Query mythenquai at 2019-07-31\n",
      "Handle mythenquai from 2019-07-30 22:10:00+00:00 to 2019-07-31 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-07-31\n",
      "Handle tiefenbrunnen from 2019-07-30 22:10:00+00:00 to 2019-07-31 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-01\n",
      "Handle mythenquai from 2019-07-31 22:10:00+00:00 to 2019-08-01 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tiefenbrunnen from 2019-07-31 22:10:00+00:00 to 2019-08-01 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-02\n",
      "Handle mythenquai from 2019-08-01 22:10:00+00:00 to 2019-08-02 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-02\n",
      "Handle tiefenbrunnen from 2019-08-01 22:10:00+00:00 to 2019-08-02 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-03\n",
      "Handle mythenquai from 2019-08-02 22:10:00+00:00 to 2019-08-03 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-03\n",
      "Handle tiefenbrunnen from 2019-08-02 22:10:00+00:00 to 2019-08-03 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-04\n",
      "Handle mythenquai from 2019-08-03 22:10:00+00:00 to 2019-08-04 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-04\n",
      "Handle tiefenbrunnen from 2019-08-03 22:10:00+00:00 to 2019-08-04 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-05\n",
      "Handle mythenquai from 2019-08-04 22:10:00+00:00 to 2019-08-05 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-05\n",
      "Handle tiefenbrunnen from 2019-08-04 22:10:00+00:00 to 2019-08-05 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-06\n",
      "Handle mythenquai from 2019-08-05 22:10:00+00:00 to 2019-08-06 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-06\n",
      "Handle tiefenbrunnen from 2019-08-05 22:10:00+00:00 to 2019-08-06 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-07\n",
      "Handle mythenquai from 2019-08-06 22:10:00+00:00 to 2019-08-07 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-07\n",
      "Handle tiefenbrunnen from 2019-08-06 22:10:00+00:00 to 2019-08-07 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-08\n",
      "Handle mythenquai from 2019-08-07 22:10:00+00:00 to 2019-08-08 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-08\n",
      "Handle tiefenbrunnen from 2019-08-07 22:10:00+00:00 to 2019-08-08 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-09\n",
      "Handle mythenquai from 2019-08-08 22:10:00+00:00 to 2019-08-09 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-09\n",
      "Handle tiefenbrunnen from 2019-08-08 22:10:00+00:00 to 2019-08-09 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-10\n",
      "Handle mythenquai from 2019-08-09 22:10:00+00:00 to 2019-08-10 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-10\n",
      "Handle tiefenbrunnen from 2019-08-09 22:10:00+00:00 to 2019-08-10 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-11\n",
      "Handle mythenquai from 2019-08-10 22:10:00+00:00 to 2019-08-11 22:00:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-11\n",
      "Handle tiefenbrunnen from 2019-08-10 22:10:00+00:00 to 2019-08-11 22:00:00+00:00\n",
      "Query mythenquai at 2019-08-12\n",
      "Handle mythenquai from 2019-08-11 22:10:00+00:00 to 2019-08-12 11:30:00+00:00\n",
      "Query tiefenbrunnen at 2019-08-12\n",
      "Handle tiefenbrunnen from 2019-08-11 22:10:00+00:00 to 2019-08-12 10:20:00+00:00\n",
      "Query mythenquai at 2019-08-12\n",
      "No new data received for mythenquai\n",
      "Query tiefenbrunnen at 2019-08-12\n",
      "No new data received for tiefenbrunnen\n",
      "Sleep for 600.0s (from 2019-08-12 11:36:32.451726+00:00 until 2019-08-12 11:46:32.451726+00:00) when next data will be queried.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Start load data code block\n",
    "\n",
    "config = Config()\n",
    "# define CSV path\n",
    "config.historic_data_folder='..'+os.sep+'data'\n",
    "\n",
    "# connect to DB\n",
    "connect_db(config)\n",
    "# clean DB\n",
    "clean_db(config)\n",
    "#client.get_list_database()\n",
    "import_historic_data(config)\n",
    "import_latest_data(config, append_to_csv=True, periodic_read=True)\n",
    "\n",
    "# End load data code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
