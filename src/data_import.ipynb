{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import tzlocal\n",
    "import pytz\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    db_host='localhost'\n",
    "    db_port=8086\n",
    "    db_name='meteorology'\n",
    "    stations = ['mythenquai', 'tiefenbrunnen']\n",
    "    keys_mapping = {\n",
    "        \"values.air_temperature.value\": \"air_temperature\",\n",
    "        \"values.barometric_pressure_qfe.value\": \"barometric_pressure_qfe\",\n",
    "        \"values.dew_point.value\": \"dew_point\",\n",
    "        \"values.global_radiation.value\": \"global_radiation\",\n",
    "        \"values.humidity.value\": \"humidity\",\n",
    "        \"values.precipitation.value\": \"precipitation\",\n",
    "        \"values.timestamp_cet.value\": \"timestamp_cet\",\n",
    "        \"values.water_temperature.value\": \"water_temperature\",\n",
    "        \"values.wind_direction.value\": \"wind_direction\",\n",
    "        \"values.wind_force_avg_10min.value\": \"wind_force_avg_10min\",\n",
    "        \"values.wind_gust_max_10min.value\": \"wind_gust_max_10min\",\n",
    "        \"values.wind_speed_avg_10min.value\": \"wind_speed_avg_10min\",\n",
    "        \"values.windchill.value\": \"windchill\"\n",
    "    }\n",
    "    historic_data_folder = '.'+os.sep+'data'\n",
    "    client = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_last_db_entry(config, station):\n",
    "    query = \"SELECT * FROM \\\"{}\\\" ORDER BY time DESC LIMIT 1\".format(station)\n",
    "    last = config.client.query(query)\n",
    "    return last\n",
    "    \n",
    "def __extract_last_db_day(last_entry, station, default_last_db_day):\n",
    "    val = last_entry[station]\n",
    "          \n",
    "    if val is not None: \n",
    "        if not val.index.empty:\n",
    "            return val.index[0]\n",
    "        \n",
    "    return default_last_db_day\n",
    "\n",
    "def __get_data_of_day(day, station):\n",
    "    base_url = 'https://tecdottir.herokuapp.com/measurements/{}'\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(\"Query \"+ station +\" at \"+day_str)\n",
    "    payload = {'startDate': day_str, 'endDate': day_str}\n",
    "    url = base_url.format(station)\n",
    "    response = requests.get(url, params=payload)\n",
    "    if(response.ok):\n",
    "        #print(response.json())\n",
    "        jData = json.loads(response.content)\n",
    "        return jData\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "def __define_types(data, date_format):\n",
    "    data['timestamp_cet'] = pd.to_datetime(data['timestamp_cet'], format=date_format)\n",
    "    # set the correct timezone\n",
    "    #data['timestamp_cet'] = data['timestamp_cet'].dt.tz_localize('Europe/Zurich')\n",
    "    data.set_index('timestamp_cet', inplace=True)\n",
    "    \n",
    "    for column in data.columns[0:]:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def __clean_data(config, data_of_last_day, last_db_entry, date_format, station):\n",
    "    normalized = json_normalize(data_of_last_day['result'])\n",
    "    \n",
    "    for column in normalized.columns[0:]:   \n",
    "        mapping = config.keys_mapping.get(column, None)\n",
    "        if mapping is not None:\n",
    "            normalized[mapping] = normalized[column]\n",
    "            \n",
    "        normalized.drop(columns=column, inplace=True)\n",
    "    \n",
    "    # make sure types/index are correct\n",
    "    normalized = __define_types(normalized, date_format)\n",
    "    \n",
    "    #print(\"Normalized index \"+str(normalized.index))\n",
    "    #print(\"Last db index \"+str(lastDBEntry[station].index))\n",
    "    \n",
    "    # remove all entries older than last element\n",
    "    last_db_entry_time = last_db_entry[station].index[0].replace(tzinfo=None)   \n",
    "    normalized.drop(normalized[normalized.index <= last_db_entry_time].index, inplace=True)\n",
    "    \n",
    "    return normalized\n",
    "        \n",
    "def __add_data_to_db(config, data, station):\n",
    "    config.client.write_points(data, station, time_precision='s', database=config.db_name)\n",
    "    \n",
    "def __append_df_to_csv(data, csv_file_path, sep=\",\"):\n",
    "    header = False\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        header = True\n",
    "\n",
    "    data.to_csv(csv_file_path, mode='a', sep=sep, header=header)\n",
    "    \n",
    "def __signal_handler(sig, frame):\n",
    "    sys.exit(0)\n",
    "    \n",
    "def connect_db(config):\n",
    "    \"\"\"Connects to the database and initializes the client\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info\n",
    "\n",
    "   \"\"\"\n",
    "    if config.client is None:\n",
    "        # https://www.influxdata.com/blog/getting-started-python-influxdb/\n",
    "        config.client = DataFrameClient(host=config.db_host, port=config.db_port, database=config.db_name)\n",
    "        config.client.switch_database(config.db_name)\n",
    "\n",
    "def clean_db(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    config.client.drop_database(config.db_name)\n",
    "    config.client.create_database(config.db_name)\n",
    "\n",
    "def import_historic_data(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    # read historic data from files\n",
    "    chunksize = 10000\n",
    "    \n",
    "    for station in config.stations:\n",
    "        last_entry = __get_last_db_entry(config, station)\n",
    "          \n",
    "        if last_entry is None or not last_entry:\n",
    "            print(\"Load historic data for \"+station + \" ...\")\n",
    "        \n",
    "            file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_2007-2018.csv\")\n",
    "            if os.path.isfile(file_name):\n",
    "                print(\"\\tLoad \"+ file_name)\n",
    "                for chunk in pd.read_csv(file_name, delimiter=',', chunksize=chunksize):\n",
    "                    chunk = __define_types(chunk, '%Y-%m-%dT%H:%M:%S')\n",
    "                    __add_data_to_db(config, chunk, station)\n",
    "            else:\n",
    "                print(file_name +\" does not seem to exist.\")\n",
    "                \n",
    "            current_time = datetime.datetime.now(tzlocal.get_localzone())\n",
    "            running_year = 2019\n",
    "            while running_year <= current_time.year:\n",
    "                file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(running_year) +\".csv\")\n",
    "                if os.path.isfile(file_name):\n",
    "                    print(\"\\tLoad \"+ file_name)\n",
    "                    for chunk in pd.read_csv(file_name, delimiter=',', chunksize=chunksize):\n",
    "                        chunk = __define_types(chunk, '%Y-%m-%d %H:%M:%S')\n",
    "                        __add_data_to_db(config, chunk, station)\n",
    "                else:\n",
    "                    print(file_name +\" does not seem to exist.\")\n",
    "                running_year+=1\n",
    "            \n",
    "    \n",
    "        print(\"Historic data for \"+station+\" loaded.\")\n",
    "        \n",
    "        \n",
    "def import_latest_data(config, append_to_csv=False, periodic_read=False):\n",
    "    \"\"\"Reads the latest data from the Wasserschutzpolizei Zurich weather API\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "    append_to_csv (bool): Defines if the data should be appended to a CSV file\n",
    "    periodic_read (bool): Defines if the function should keep reading after it imported the latest data (blocking through a sleep)\n",
    "\n",
    "   \"\"\"\n",
    "    # access API for current data\n",
    "    current_time = datetime.datetime.now(pytz.utc)\n",
    "    last_api_call = current_time.replace(second=0, microsecond=0) - datetime.timedelta(minutes=10)\n",
    "    last_db_days = [last_api_call] * len(config.stations)\n",
    "    new_data_received = True\n",
    "\n",
    "    for idx, station in enumerate(config.stations):\n",
    "        last_db_entry = __get_last_db_entry(config, station)\n",
    "        last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "\n",
    "    if periodic_read:\n",
    "        signal.signal(signal.SIGINT, __signal_handler)\n",
    "        print(\"\\nPress Ctrl+C to stop!\\n\")\n",
    "\n",
    "    while True:\n",
    "        current_time = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "        # check if all historic data (retrieved from API) has been processed \n",
    "        if periodic_read and max(last_db_days) >= last_api_call: \n",
    "            # once every 10 Min\n",
    "            sleep_until = current_time + datetime.timedelta(minutes=10)       \n",
    "            # once per day\n",
    "            # sleep_until = current_time + datetime.timedelta(days=1)\n",
    "            # sleep_until = sleep_until.replace(hour=6, minute=0, second=0, microsecond=0)\n",
    "            sleep_sec = (sleep_until - current_time).total_seconds()\n",
    "\n",
    "            print(\"Sleep for \"+str(sleep_sec) + \"s (from \" + str(current_time) +\" until \"+str(sleep_until) + \") when next data will be queried.\")\n",
    "            sleep(sleep_sec)\n",
    "            current_time = datetime.datetime.now(pytz.utc)\n",
    "            last_api_call = current_time.replace(second=0, microsecond=0)\n",
    "        elif not periodic_read and not new_data_received:\n",
    "            # stop here\n",
    "            return;\n",
    "\n",
    "        new_data_received = False\n",
    "        for idx, station in enumerate(config.stations):\n",
    "            last_db_entry = __get_last_db_entry(config, station)\n",
    "            last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "            data_of_last_db_day = __get_data_of_day(last_db_days[idx], station)\n",
    "            normalized_data = __clean_data(config, data_of_last_db_day, last_db_entry, '%d.%m.%Y %H:%M:%S', station)\n",
    "\n",
    "            if normalized_data.size > 0:\n",
    "                new_data_received = True\n",
    "                __add_data_to_db(config, normalized_data, station)\n",
    "                if append_to_csv:\n",
    "                    __append_df_to_csv(normalized_data, os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(current_time.year) +\".csv\"))\n",
    "                print(\"Handle \"+ station +\" from \"+ str(normalized_data.index[0]) +\" to \"+ str(normalized_data.index[-1])) \n",
    "            else:\n",
    "                print(\"No new data received for \"+station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load historic data for mythenquai ...\n",
      "\tLoad ../data/messwerte_mythenquai_2007-2018.csv\n",
      "\tLoad ../data/messwerte_mythenquai_2019.csv\n",
      "Historic data for mythenquai loaded.\n",
      "Load historic data for tiefenbrunnen ...\n",
      "\tLoad ../data/messwerte_tiefenbrunnen_2007-2018.csv\n",
      "\tLoad ../data/messwerte_tiefenbrunnen_2019.csv\n",
      "Historic data for tiefenbrunnen loaded.\n",
      "\n",
      "Press Ctrl+C to stop!\n",
      "\n",
      "Sleep for 600.0s (from 2019-06-27 07:56:26.123936+00:00 until 2019-06-27 08:06:26.123936+00:00) when next data will be queried.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Start load data code block\n",
    "\n",
    "config = Config()\n",
    "# define CSV path\n",
    "config.historic_data_folder='..'+os.sep+'data'\n",
    "\n",
    "# connect to DB\n",
    "connect_db(config)\n",
    "# clean DB\n",
    "clean_db(config)\n",
    "#client.get_list_database()\n",
    "import_historic_data(config)\n",
    "import_latest_data(config, append_to_csv=True, periodic_read=True)\n",
    "\n",
    "# End load data code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
