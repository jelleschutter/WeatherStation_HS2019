{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import tzlocal\n",
    "import pytz\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    db_host='localhost'\n",
    "    db_port=8086\n",
    "    db_name='meteorology'\n",
    "    stations = ['mythenquai', 'tiefenbrunnen']\n",
    "    stations_force_query_last_entry = False\n",
    "    stations_last_entries = {}\n",
    "    keys_mapping = {\n",
    "        \"values.air_temperature.value\": \"air_temperature\",\n",
    "        \"values.barometric_pressure_qfe.value\": \"barometric_pressure_qfe\",\n",
    "        \"values.dew_point.value\": \"dew_point\",\n",
    "        \"values.global_radiation.value\": \"global_radiation\",\n",
    "        \"values.humidity.value\": \"humidity\",\n",
    "        \"values.precipitation.value\": \"precipitation\",\n",
    "        \"values.timestamp_cet.value\": \"timestamp_cet\",\n",
    "        \"values.water_temperature.value\": \"water_temperature\",\n",
    "        \"values.wind_direction.value\": \"wind_direction\",\n",
    "        \"values.wind_force_avg_10min.value\": \"wind_force_avg_10min\",\n",
    "        \"values.wind_gust_max_10min.value\": \"wind_gust_max_10min\",\n",
    "        \"values.wind_speed_avg_10min.value\": \"wind_speed_avg_10min\",\n",
    "        \"values.windchill.value\": \"windchill\"\n",
    "    }\n",
    "    historic_data_folder = '.'+os.sep+'data'\n",
    "    historic_data_chunksize = 10000\n",
    "    historic_data_sleep_sec = 0\n",
    "    client = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __set_last_db_entry(config, station, entry):\n",
    "    current_last_time = __extract_last_db_day(config.stations_last_entries.get(station, None), station, None)\n",
    "    entry_time = __extract_last_db_day(entry, station, None)\n",
    "\n",
    "    if current_last_time is None and entry_time is not None:\n",
    "        config.stations_last_entries[station] = entry\n",
    "    elif current_last_time is not None and entry_time is not None and current_last_time < entry_time:\n",
    "        config.stations_last_entries[station] = entry\n",
    "\n",
    "def __get_last_db_entry(config, station):\n",
    "    last_entry = None\n",
    "    if not config.stations_force_query_last_entry:\n",
    "        # speedup for Raspberry Pi - last entry query takes > 2 Sec.!\n",
    "        last_entry = config.stations_last_entries.get(station, None)\n",
    "    \n",
    "    if last_entry is None:\n",
    "        try:\n",
    "            # we are only interested in time, however need to provide any field to make query work\n",
    "            query = \"SELECT last(air_temperature) FROM \\\"{}\\\"\".format(station)\n",
    "            last_entry = config.client.query(query)\n",
    "        except:\n",
    "            # There are influxDB versions which have an issue with above \"last\" query\n",
    "            print(\"An exception occurred while querying last entry from DB for \"+ station +\". Try alternative approach.\") \n",
    "            query = \"SELECT * FROM \\\"{}\\\" ORDER BY time DESC LIMIT 1\".format(station)\n",
    "            last_entry = config.client.query(query) \n",
    "        \n",
    "    __set_last_db_entry(config, station, last_entry)\n",
    "    return last_entry\n",
    "    \n",
    "def __extract_last_db_day(last_entry, station, default_last_db_day):\n",
    "    if last_entry is not None:\n",
    "        val = None\n",
    "        if isinstance(last_entry, pd.DataFrame):\n",
    "            val = last_entry\n",
    "        elif isinstance(last_entry, dict):\n",
    "            val = last_entry.get(station, None)   \n",
    "            \n",
    "        if val is not None: \n",
    "            if not val.index.empty:\n",
    "                return val.index[0]\n",
    "        \n",
    "    return default_last_db_day\n",
    "\n",
    "def __get_data_of_day(day, station):\n",
    "    # convert to local time of station\n",
    "    day = day.tz_convert('Europe/Zurich')\n",
    "    base_url = 'https://tecdottir.herokuapp.com/measurements/{}'\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(\"Query \"+ station +\" at \"+day_str)\n",
    "    payload = {'startDate': day_str, 'endDate': day_str}\n",
    "    url = base_url.format(station)\n",
    "    response = requests.get(url, params=payload)\n",
    "    if(response.ok):\n",
    "        #print(response.json())\n",
    "        jData = json.loads(response.content)\n",
    "        return jData\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "def __define_types(data, date_format):\n",
    "    data['timestamp_cet'] = pd.to_datetime(data['timestamp_cet'], format=date_format)\n",
    "    if not data.empty and data['timestamp_cet'].iloc[0].tzinfo is None:\n",
    "        data['timestamp_cet'] = data['timestamp_cet'].dt.tz_localize('Europe/Zurich', ambiguous=True).dt.tz_convert('UTC')\n",
    "    data.set_index('timestamp_cet', inplace=True)\n",
    "    \n",
    "    for column in data.columns[0:]:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def __clean_data(config, data_of_last_day, last_db_entry, date_format, station):\n",
    "    normalized = json_normalize(data_of_last_day['result'])\n",
    "    \n",
    "    for column in normalized.columns[0:]:   \n",
    "        mapping = config.keys_mapping.get(column, None)\n",
    "        if mapping is not None:\n",
    "            normalized[mapping] = normalized[column]\n",
    "            \n",
    "        normalized.drop(columns=column, inplace=True)\n",
    "    \n",
    "    # make sure types/index are correct\n",
    "    normalized = __define_types(normalized, date_format)\n",
    "    \n",
    "    #print(\"Normalized index \"+str(normalized.index))\n",
    "    #print(\"Last db index \"+str(lastDBEntry[station].index))\n",
    "    \n",
    "    # remove all entries older than last element\n",
    "    last_db_entry_time = None\n",
    "    if isinstance(last_db_entry, pd.DataFrame):\n",
    "        last_db_entry_time = last_db_entry\n",
    "    elif isinstance(last_db_entry, dict):\n",
    "        last_db_entry_time = last_db_entry.get(station, None) \n",
    "    last_db_entry_time = last_db_entry_time.index[0] #.replace(tzinfo=None)\n",
    "    # print(\"Last \"+str(last_db_entry_time) +\" elements \"+str(normalized.index[0]) +\" - \"+str(normalized.index[-1]))\n",
    "    normalized.drop(normalized[normalized.index <= last_db_entry_time].index, inplace=True)\n",
    "    \n",
    "    return normalized\n",
    "        \n",
    "def __add_data_to_db(config, data, station):\n",
    "    config.client.write_points(data, station, time_precision='s', database=config.db_name)\n",
    "    __set_last_db_entry(config, station, data.tail(1))\n",
    "    \n",
    "def __append_df_to_csv(data, csv_file_path, sep=\",\"):\n",
    "    header = False\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        header = True\n",
    "\n",
    "    data.to_csv(csv_file_path, mode='a', sep=sep, header=header)\n",
    "    \n",
    "def __signal_handler(sig, frame):\n",
    "    sys.exit(0)\n",
    "    \n",
    "def connect_db(config):\n",
    "    \"\"\"Connects to the database and initializes the client\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info\n",
    "\n",
    "   \"\"\"\n",
    "    if config.client is None:\n",
    "        # https://www.influxdata.com/blog/getting-started-python-influxdb/\n",
    "        config.client = DataFrameClient(host=config.db_host, port=config.db_port, database=config.db_name)\n",
    "        config.client.switch_database(config.db_name)\n",
    "\n",
    "def clean_db(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    config.client.drop_database(config.db_name)\n",
    "    config.client.create_database(config.db_name)\n",
    "    config.stations_last_entries.clear()\n",
    "\n",
    "def import_historic_data(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    # read historic data from files\n",
    "    \n",
    "    for station in config.stations:\n",
    "        last_entry = __get_last_db_entry(config, station)\n",
    "          \n",
    "        if last_entry is None or not last_entry:\n",
    "            print(\"Load historic data for \"+station + \" ...\")\n",
    "        \n",
    "            file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_2007-2018.csv\")\n",
    "            if os.path.isfile(file_name):\n",
    "                print(\"\\tLoad \"+ file_name)\n",
    "                for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                    chunk = __define_types(chunk, '%Y-%m-%dT%H:%M:%S')\n",
    "                    print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                    __add_data_to_db(config, chunk, station)\n",
    "                    \n",
    "                    if config.historic_data_sleep_sec > 0:\n",
    "                        sleep(config.historic_data_sleep_sec)\n",
    "            else:\n",
    "                print(file_name +\" does not seem to exist.\")\n",
    "                \n",
    "            current_time = datetime.datetime.now(tzlocal.get_localzone())\n",
    "            running_year = 2019\n",
    "            while running_year <= current_time.year:\n",
    "                file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(running_year) +\".csv\")\n",
    "                if os.path.isfile(file_name):\n",
    "                    print(\"\\tLoad \"+ file_name)\n",
    "                    for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                        chunk = __define_types(chunk, '%Y-%m-%d %H:%M:%S')\n",
    "                        print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                        __add_data_to_db(config, chunk, station)\n",
    "                        \n",
    "                        if config.historic_data_sleep_sec > 0:\n",
    "                            sleep(config.historic_data_sleep_sec)\n",
    "                else:\n",
    "                    print(file_name +\" does not seem to exist.\")\n",
    "                running_year+=1\n",
    "        else:\n",
    "            print(\"There is already data for \"+station + \". No historic data will be imported.\")\n",
    "    \n",
    "        print(\"Historic data for \"+station+\" loaded.\")\n",
    "        \n",
    "        \n",
    "def import_latest_data(config, append_to_csv=False, periodic_read=False):\n",
    "    \"\"\"Reads the latest data from the Wasserschutzpolizei Zurich weather API\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "    append_to_csv (bool): Defines if the data should be appended to a CSV file\n",
    "    periodic_read (bool): Defines if the function should keep reading after it imported the latest data (blocking through a sleep)\n",
    "\n",
    "   \"\"\"\n",
    "    # access API for current data\n",
    "    current_time = datetime.datetime.now(pytz.utc)\n",
    "    current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    last_db_days = [current_day] * len(config.stations)\n",
    "    new_data_received = True\n",
    "\n",
    "    for idx, station in enumerate(config.stations):\n",
    "        last_db_entry = __get_last_db_entry(config, station)\n",
    "        last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "\n",
    "    if periodic_read:\n",
    "        signal.signal(signal.SIGINT, __signal_handler)\n",
    "        print(\"\\nPress Ctrl+C to stop!\\n\")\n",
    "\n",
    "    while True:\n",
    "        current_time = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "        # check if all historic data (retrieved from API) has been processed \n",
    "        last_db_day = max(last_db_days)\n",
    "        if periodic_read and last_db_day >= current_day: \n",
    "            # once every 10 Min\n",
    "            sleep_until = current_time + datetime.timedelta(minutes=10)       \n",
    "            # once per day\n",
    "            # sleep_until = current_time + datetime.timedelta(days=1)\n",
    "            # sleep_until = sleep_until.replace(hour=6, minute=0, second=0, microsecond=0)\n",
    "            sleep_sec = (sleep_until - current_time).total_seconds()\n",
    "\n",
    "            print(\"Sleep for \"+str(sleep_sec) + \"s (from \" + str(current_time) +\" until \"+str(sleep_until) + \") when next data will be queried.\")\n",
    "            sleep(sleep_sec)\n",
    "            current_time = datetime.datetime.now(pytz.utc)\n",
    "            current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        elif not periodic_read and not new_data_received:\n",
    "            # stop here\n",
    "            return;\n",
    "\n",
    "        new_data_received = False\n",
    "        for idx, station in enumerate(config.stations):\n",
    "            last_db_entry = __get_last_db_entry(config, station)\n",
    "            last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "            data_of_last_db_day = __get_data_of_day(last_db_days[idx], station)\n",
    "\n",
    "            normalized_data = __clean_data(config, data_of_last_db_day, last_db_entry, '%d.%m.%Y %H:%M:%S', station)\n",
    "\n",
    "            if normalized_data.size > 0:\n",
    "                new_data_received = True\n",
    "                __add_data_to_db(config, normalized_data, station)\n",
    "                if append_to_csv:\n",
    "                    __append_df_to_csv(normalized_data, os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(current_time.year) +\".csv\"))\n",
    "                print(\"Handle \"+ station +\" from \"+ str(normalized_data.index[0]) +\" to \"+ str(normalized_data.index[-1])) \n",
    "            else:\n",
    "                print(\"No new data received for \"+station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load historic data for mythenquai ...\n",
      "\tLoad ../data/messwerte_mythenquai_2007-2018.csv\n",
      "Add mythenquai from 2007-04-22 19:20:00+00:00 to 2007-07-01 05:50:00+00:00\n",
      "Add mythenquai from 2007-07-01 06:00:00+00:00 to 2007-09-08 16:30:00+00:00\n",
      "Add mythenquai from 2007-09-08 16:40:00+00:00 to 2007-11-17 04:10:00+00:00\n",
      "Add mythenquai from 2007-11-17 04:20:00+00:00 to 2008-01-25 14:50:00+00:00\n",
      "Add mythenquai from 2008-01-25 15:00:00+00:00 to 2008-04-04 01:40:00+00:00\n",
      "Add mythenquai from 2008-04-04 01:50:00+00:00 to 2008-06-12 13:50:00+00:00\n",
      "Add mythenquai from 2008-06-12 14:00:00+00:00 to 2008-08-21 02:20:00+00:00\n",
      "Add mythenquai from 2008-08-21 02:30:00+00:00 to 2008-10-29 16:30:00+00:00\n",
      "Add mythenquai from 2008-10-29 16:40:00+00:00 to 2009-01-07 13:40:00+00:00\n",
      "Add mythenquai from 2009-01-07 13:50:00+00:00 to 2009-03-18 17:20:00+00:00\n",
      "Add mythenquai from 2009-03-18 17:30:00+00:00 to 2009-05-27 20:50:00+00:00\n",
      "Add mythenquai from 2009-05-27 21:00:00+00:00 to 2009-08-05 12:00:00+00:00\n",
      "Add mythenquai from 2009-08-05 12:10:00+00:00 to 2009-10-13 22:50:00+00:00\n",
      "Add mythenquai from 2009-10-13 23:00:00+00:00 to 2009-12-22 10:50:00+00:00\n",
      "Add mythenquai from 2009-12-22 11:00:00+00:00 to 2010-03-01 21:30:00+00:00\n",
      "Add mythenquai from 2010-03-01 21:40:00+00:00 to 2010-05-10 08:20:00+00:00\n",
      "Add mythenquai from 2010-05-10 08:30:00+00:00 to 2010-07-18 21:10:00+00:00\n",
      "Add mythenquai from 2010-07-18 21:20:00+00:00 to 2010-09-26 08:10:00+00:00\n",
      "Add mythenquai from 2010-09-26 08:20:00+00:00 to 2011-01-23 00:10:00+00:00\n",
      "Add mythenquai from 2011-01-23 00:20:00+00:00 to 2011-04-02 14:40:00+00:00\n",
      "Add mythenquai from 2011-04-02 14:50:00+00:00 to 2011-06-11 01:20:00+00:00\n",
      "Add mythenquai from 2011-06-11 01:30:00+00:00 to 2011-08-20 14:30:00+00:00\n",
      "Add mythenquai from 2011-08-20 14:40:00+00:00 to 2011-10-29 01:40:00+00:00\n",
      "Add mythenquai from 2011-10-29 01:50:00+00:00 to 2012-01-06 17:20:00+00:00\n",
      "Add mythenquai from 2012-01-06 17:30:00+00:00 to 2012-03-16 09:10:00+00:00\n",
      "Add mythenquai from 2012-03-16 09:20:00+00:00 to 2012-05-24 21:10:00+00:00\n",
      "Add mythenquai from 2012-05-24 21:20:00+00:00 to 2012-08-02 08:00:00+00:00\n",
      "Add mythenquai from 2012-08-02 08:10:00+00:00 to 2012-10-10 20:30:00+00:00\n",
      "Add mythenquai from 2012-10-10 20:40:00+00:00 to 2012-12-19 09:00:00+00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7087782b4e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclean_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#client.get_list_database()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mimport_historic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mimport_latest_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_to_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiodic_read\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61b1db6d47d6>\u001b[0m in \u001b[0;36mimport_historic_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tLoad \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistoric_data_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__define_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%dT%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Add \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstation\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" from \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" to \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0m__add_data_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61b1db6d47d6>\u001b[0m in \u001b[0;36m__define_types\u001b[0;34m(data, date_format)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtzinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Europe/Zurich'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp_cet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m_delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/datetimelike.py\u001b[0m in \u001b[0;36m_delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_delegate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethodcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mtz_localize\u001b[0;34m(self, tz, ambiguous, nonexistent, errors)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             new_dates = conversion.tz_localize_to_utc(\n\u001b[0;32m-> 1055\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masi8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonexistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnonexistent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             )\n\u001b[1;32m   1057\u001b[0m         \u001b[0mnew_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NS_DTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start load data code block\n",
    "\n",
    "config = Config()\n",
    "# define CSV path\n",
    "config.historic_data_folder='..'+os.sep+'data'\n",
    "\n",
    "# connect to DB\n",
    "connect_db(config)\n",
    "# clean DB\n",
    "clean_db(config)\n",
    "#client.get_list_database()\n",
    "import_historic_data(config)\n",
    "import_latest_data(config, append_to_csv=True, periodic_read=True)\n",
    "\n",
    "# End load data code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
