{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import tzlocal\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "db_name='meteorology'\n",
    "stations = ['mythenquai', 'tiefenbrunnen']\n",
    "keysMapping = {\n",
    "    \"values.air_temperature.value\": \"air_temperature\",\n",
    "    \"values.barometric_pressure_qfe.value\": \"barometric_pressure_qfe\",\n",
    "    \"values.dew_point.value\": \"dew_point\",\n",
    "    \"values.global_radiation.value\": \"global_radiation\",\n",
    "    \"values.humidity.value\": \"humidity\",\n",
    "    \"values.precipitation.value\": \"precipitation\",\n",
    "    \"values.timestamp_cet.value\": \"timestamp_cet\",\n",
    "    \"values.water_temperature.value\": \"water_temperature\",\n",
    "    \"values.wind_direction.value\": \"wind_direction\",\n",
    "    \"values.wind_force_avg_10min.value\": \"wind_force_avg_10min\",\n",
    "    \"values.wind_gust_max_10min.value\": \"wind_gust_max_10min\",\n",
    "    \"values.wind_speed_avg_10min.value\": \"wind_speed_avg_10min\",\n",
    "    \"values.windchill.value\": \"windchill\"\n",
    "}\n",
    "# https://www.influxdata.com/blog/getting-started-python-influxdb/\n",
    "client = DataFrameClient(host='localhost', port=8086, database=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDB(client, dbName):\n",
    "    client.drop_database(dbName)\n",
    "    client.create_database(dbName)\n",
    "\n",
    "def getLastDBEntry(client, station):\n",
    "    query = \"SELECT * FROM \\\"{}\\\" ORDER BY time DESC LIMIT 1\".format(station)\n",
    "    last = client.query(query)\n",
    "    return last\n",
    "    \n",
    "def extractLastDBDay(lastEntry, station):\n",
    "    lastDay = lastEntry[station].index\n",
    "    return lastDay[0]\n",
    "\n",
    "def getDataOfDay(station, day):\n",
    "    base_url = 'https://tecdottir.herokuapp.com/measurements/{}'\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(\"Query \"+ station +\" at \"+day_str)\n",
    "    payload = {'startDate': day_str, 'endDate': day_str}\n",
    "    url = base_url.format(station)\n",
    "    response = requests.get(url, params=payload)\n",
    "    if(response.ok):\n",
    "        #print(response.json())\n",
    "        jData = json.loads(response.content)\n",
    "        return jData\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "def defineTypes(data, dateFormat):\n",
    "    data['timestamp_cet'] = pd.to_datetime(data['timestamp_cet'], format=dateFormat)\n",
    "    # set the correct timezone\n",
    "    #data['timestamp_cet'] = data['timestamp_cet'].dt.tz_localize('Europe/Zurich')\n",
    "    data.set_index('timestamp_cet', inplace=True)\n",
    "    \n",
    "    for column in data.columns[0:]:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def cleanData(keysMapping, dataOfLastDay, lastDBEntry, station, dateFormat):\n",
    "    normalized = json_normalize(dataOfLastDay['result'])\n",
    "    \n",
    "    for column in normalized.columns[0:]:   \n",
    "        mapping = keysMapping.get(column, None)\n",
    "        if mapping is not None:\n",
    "            normalized[mapping] = normalized[column]\n",
    "            \n",
    "        normalized.drop(columns=column, inplace=True)\n",
    "    \n",
    "    # make sure types/index are correct\n",
    "    defineTypes(normalized, dateFormat)\n",
    "    \n",
    "    #print(\"Normalized index \"+str(normalized.index))\n",
    "    #print(\"Last db index \"+str(lastDBEntry[station].index))\n",
    "    \n",
    "    # remove all entries older than last element\n",
    "    #normalized.drop(normalized[normalized.index <= lastDBEntry[station].index].index, inplace=True)\n",
    "    \n",
    "    return normalized\n",
    "        \n",
    "def addDataToDB(client, data, station, dbName):\n",
    "    client.write_points(data, station, time_precision='s', database=dbName)\n",
    "    \n",
    "def appendDFToCSV(data, csvFilePath, sep=\",\"):\n",
    "    header = False\n",
    "    if not os.path.isfile(csvFilePath):\n",
    "        header = True\n",
    "\n",
    "    data.to_csv(csvFilePath, mode='a', sep=sep, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _signal.default_int_handler>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def signal_handler(sig, frame):\n",
    "        print('You pressed Ctrl+C!')\n",
    "        sys.exit(0)\n",
    "signal.signal(signal.SIGINT, signal_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean DB\n",
    "cleanDB(client, db_name)\n",
    "client.switch_database(db_name)\n",
    "#client.get_list_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historic data for mythenquai loaded.\n",
      "Historic data for tiefenbrunnen loaded.\n"
     ]
    }
   ],
   "source": [
    "# read historic data from files\n",
    "chunksize = 10000\n",
    "    \n",
    "for station in stations:\n",
    "    lastEntry = getLastDBEntry(client, station)\n",
    "          \n",
    "    if lastEntry is None or len(lastEntry) == 0:\n",
    "        print(\"Load historic data for \"+station + \" ...\")\n",
    "        \n",
    "        for chunk in pd.read_csv(\"./data/messwerte_\" + station + \"_2007-2018.csv\", delimiter=',', chunksize=chunksize):\n",
    "            defineTypes(chunk, '%Y-%m-%dT%H:%M:%S')\n",
    "            addDataToDB(client, chunk, station, db_name)\n",
    "\n",
    "        for chunk in pd.read_csv(\"./data/messwerte_\" + station + \"_2019.csv\", delimiter=',', chunksize=chunksize):\n",
    "            defineTypes(chunk, '%Y-%m-%d %H:%M:%S')\n",
    "            addDataToDB(client, chunk, station, db_name)\n",
    "            \n",
    "    \n",
    "    print(\"Historic data for \"+station+\" loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep for 57047.692623 from 2019-06-12 14:09:12.307377+00:00 until 2019-06-13 06:00:00+00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f5900ee0839b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep for \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleepSec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" from \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentTime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" until \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleepUntil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleepSec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mcurrentDayStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_localzone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcurrentDayStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentDayStart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmicrosecond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# access API for current data\n",
    "currentDayStart = datetime.datetime.now(tzlocal.get_localzone())\n",
    "currentDayStart = currentDayStart.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "lastDBDays = [datetime.datetime.strptime(\"2018-01-01\", '%Y-%m-%d')] * len(stations)\n",
    "\n",
    "for idx, station in enumerate(stations):\n",
    "    lastDBEntry = getLastDBEntry(client, station)\n",
    "    lastDBDays[idx] = extractLastDBDay(lastDBEntry, station)\n",
    "    \n",
    "while True:\n",
    "    # check if all historic data (retrieved from API) has been processed    \n",
    "    if max(lastDBDays) >= currentDayStart:\n",
    "        currentTime = datetime.datetime.now(tzlocal.get_localzone()) \n",
    "        sleepUntil = currentTime + datetime.timedelta(days=1)\n",
    "        sleepUntil = sleepUntil.replace(hour=6, minute=0, second=0, microsecond=0)\n",
    "        sleepSec = (sleepUntil - currentTime).total_seconds()\n",
    "        \n",
    "        print(\"Sleep for \"+str(sleepSec) + \"s (from \" + str(currentTime) +\" until \"+str(sleepUntil) + \") when next data will be queried.\")\n",
    "        sleep(sleepSec)\n",
    "        currentDayStart = datetime.datetime.now(tzlocal.get_localzone())\n",
    "        currentDayStart = currentDayStart.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    for idx, station in enumerate(stations):\n",
    "        lastDBEntry = getLastDBEntry(client, station)\n",
    "        lastDBDays[idx] = extractLastDBDay(lastDBEntry, station)\n",
    "        dataOfLastDBDay = getDataOfDay(station, lastDBDays[idx])\n",
    "        normalizedData = cleanData(keysMapping, dataOfLastDBDay, lastDBEntry, station, '%d.%m.%Y %H:%M:%S')\n",
    "        addDataToDB(client, normalizedData, station, db_name)\n",
    "        appendDFToCSV(normalizedData, \"./data/messwerte_\" + station + \"_2019.csv\")\n",
    "        \n",
    "        print(\"Handle \"+ station +\" from \"+ str(normalizedData.index[0]) +\" to \"+ str(normalizedData.index[-1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
