{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import tzlocal\n",
    "import pytz\n",
    "from time import sleep\n",
    "import os\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    db_host='localhost'\n",
    "    db_port=8086\n",
    "    db_name='meteorology'\n",
    "    stations_all = ['mythenquai', 'tiefenbrunnen']\n",
    "    __stations_recorded = list(stations_all)\n",
    "    stations_force_query_last_entry = False\n",
    "    stations_last_entries = {}\n",
    "    keys_mapping = {\n",
    "        \"values.air_temperature.value\": \"air_temperature\",\n",
    "        \"values.barometric_pressure_qfe.value\": \"barometric_pressure_qfe\",\n",
    "        \"values.dew_point.value\": \"dew_point\",\n",
    "        \"values.global_radiation.value\": \"global_radiation\",\n",
    "        \"values.humidity.value\": \"humidity\",\n",
    "        \"values.precipitation.value\": \"precipitation\",\n",
    "        \"values.timestamp_cet.value\": \"timestamp_cet\",\n",
    "        \"values.water_temperature.value\": \"water_temperature\",\n",
    "        \"values.wind_direction.value\": \"wind_direction\",\n",
    "        \"values.wind_force_avg_10min.value\": \"wind_force_avg_10min\",\n",
    "        \"values.wind_gust_max_10min.value\": \"wind_gust_max_10min\",\n",
    "        \"values.wind_speed_avg_10min.value\": \"wind_speed_avg_10min\",\n",
    "        \"values.windchill.value\": \"windchill\"\n",
    "    }\n",
    "    historic_data_folder = '.'+os.sep+'data'\n",
    "    historic_data_chunksize = 10000\n",
    "    historic_data_sleep_sec = 0\n",
    "    client = None\n",
    "    lock = threading.RLock()\n",
    "    \n",
    "    def get_stations(self):\n",
    "        return self.stations_all\n",
    "    \n",
    "    def get_stations_recorded(self):\n",
    "        recorded = None\n",
    "        with self.lock:\n",
    "            recorded = list( self.__stations_recorded)\n",
    "        \n",
    "        return recorded\n",
    "    \n",
    "    def add_station(self, station):\n",
    "        if station in self.stations_all:\n",
    "            with self.lock:\n",
    "                self.__stations_recorded.append(station)\n",
    "        else:\n",
    "            raise Exception('{} is not part of the pre-configured collection of possible stations {}'.format(station, self.stations_all))\n",
    "            \n",
    "    def remove_station(self, station):\n",
    "        with self.lock:\n",
    "            self.__stations_recorded.remove(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __set_last_db_entry(config, station, entry):\n",
    "    current_last_time = __extract_last_db_day(config.stations_last_entries.get(station, None), station, None)\n",
    "    entry_time = __extract_last_db_day(entry, station, None)\n",
    "\n",
    "    if current_last_time is None and entry_time is not None:\n",
    "        config.stations_last_entries[station] = entry\n",
    "    elif current_last_time is not None and entry_time is not None and current_last_time < entry_time:\n",
    "        config.stations_last_entries[station] = entry\n",
    "\n",
    "def __get_last_db_entry(config, station):\n",
    "    last_entry = None\n",
    "    if not config.stations_force_query_last_entry:\n",
    "        # speedup for Raspberry Pi - last entry query takes > 2 Sec.!\n",
    "        last_entry = config.stations_last_entries.get(station, None)\n",
    "    \n",
    "    if last_entry is None:\n",
    "        try:\n",
    "            # we are only interested in time, however need to provide any field to make query work\n",
    "            query = \"SELECT last(air_temperature) FROM \\\"{}\\\"\".format(station)\n",
    "            last_entry = config.client.query(query)\n",
    "        except:\n",
    "            # There are influxDB versions which have an issue with above \"last\" query\n",
    "            print(\"An exception occurred while querying last entry from DB for \"+ station +\". Try alternative approach.\") \n",
    "            query = \"SELECT * FROM \\\"{}\\\" ORDER BY time DESC LIMIT 1\".format(station)\n",
    "            last_entry = config.client.query(query) \n",
    "        \n",
    "    __set_last_db_entry(config, station, last_entry)\n",
    "    return last_entry\n",
    "    \n",
    "def __extract_last_db_day(last_entry, station, default_last_db_day):\n",
    "    if last_entry is not None:\n",
    "        val = None\n",
    "        if isinstance(last_entry, pd.DataFrame):\n",
    "            val = last_entry\n",
    "        elif isinstance(last_entry, dict):\n",
    "            val = last_entry.get(station, None)   \n",
    "            \n",
    "        if val is not None: \n",
    "            if not val.index.empty:\n",
    "                return val.index[0]\n",
    "        \n",
    "    return default_last_db_day\n",
    "\n",
    "def __get_data_of_day(day, station):\n",
    "    # convert to local time of station\n",
    "    day = day.tz_convert('Europe/Zurich')\n",
    "    base_url = 'https://tecdottir.herokuapp.com/measurements/{}'\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(\"Query \"+ station +\" at \"+day_str)\n",
    "    payload = {'startDate': day_str, 'endDate': day_str}\n",
    "    url = base_url.format(station)\n",
    "    response = requests.get(url, params=payload)\n",
    "    if(response.ok):\n",
    "        #print(response.json())\n",
    "        jData = json.loads(response.content)\n",
    "        return jData\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "def __define_types(data, date_format):\n",
    "    data['timestamp_cet'] = pd.to_datetime(data['timestamp_cet'], format=date_format)\n",
    "    if not data.empty and data['timestamp_cet'].iloc[0].tzinfo is None:\n",
    "        data['timestamp_cet'] = data['timestamp_cet'].dt.tz_localize('Europe/Zurich', ambiguous=True).dt.tz_convert('UTC')\n",
    "    data.set_index('timestamp_cet', inplace=True)\n",
    "    \n",
    "    for column in data.columns[0:]:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def __clean_data(config, data_of_last_day, last_db_entry, date_format, station):\n",
    "    normalized = json_normalize(data_of_last_day['result'])\n",
    "    \n",
    "    for column in normalized.columns[0:]:   \n",
    "        mapping = config.keys_mapping.get(column, None)\n",
    "        if mapping is not None:\n",
    "            normalized[mapping] = normalized[column]\n",
    "            \n",
    "        normalized.drop(columns=column, inplace=True)\n",
    "    \n",
    "    # make sure types/index are correct\n",
    "    normalized = __define_types(normalized, date_format)\n",
    "    \n",
    "    #print(\"Normalized index \"+str(normalized.index))\n",
    "    #print(\"Last db index \"+str(lastDBEntry[station].index))\n",
    "    \n",
    "    # remove all entries older than last element\n",
    "    last_db_entry_time = None\n",
    "    if isinstance(last_db_entry, pd.DataFrame):\n",
    "        last_db_entry_time = last_db_entry\n",
    "    elif isinstance(last_db_entry, dict):\n",
    "        last_db_entry_time = last_db_entry.get(station, None) \n",
    "    last_db_entry_time = last_db_entry_time.index[0] #.replace(tzinfo=None)\n",
    "    # print(\"Last \"+str(last_db_entry_time) +\" elements \"+str(normalized.index[0]) +\" - \"+str(normalized.index[-1]))\n",
    "    normalized.drop(normalized[normalized.index <= last_db_entry_time].index, inplace=True)\n",
    "    \n",
    "    return normalized\n",
    "        \n",
    "def __add_data_to_db(config, data, station):\n",
    "    config.client.write_points(data, station, time_precision='s', database=config.db_name)\n",
    "    __set_last_db_entry(config, station, data.tail(1))\n",
    "    \n",
    "def __append_df_to_csv(data, csv_file_path, sep=\",\"):\n",
    "    header = False\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        header = True\n",
    "\n",
    "    data.to_csv(csv_file_path, mode='a', sep=sep, header=header)\n",
    "    \n",
    "def __signal_handler(sig, frame):\n",
    "    sys.exit(0)\n",
    "    \n",
    "def connect_db(config):\n",
    "    \"\"\"Connects to the database and initializes the client\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info\n",
    "\n",
    "   \"\"\"\n",
    "    if config.client is None:\n",
    "        # https://www.influxdata.com/blog/getting-started-python-influxdb/\n",
    "        config.client = DataFrameClient(host=config.db_host, port=config.db_port, database=config.db_name)\n",
    "        config.client.switch_database(config.db_name)\n",
    "\n",
    "def clean_db(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    config.client.drop_database(config.db_name)\n",
    "    config.client.create_database(config.db_name)\n",
    "    config.stations_last_entries.clear()\n",
    "\n",
    "def import_historic_data(config):\n",
    "    \"\"\"Reads the historic data of the Wasserschutzpolizei Zurich from CSV files\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "\n",
    "   \"\"\"\n",
    "    # read historic data from files\n",
    "    stations = config.get_stations_recorded()\n",
    "    \n",
    "    for station in stations:\n",
    "        last_entry = __get_last_db_entry(config, station)\n",
    "          \n",
    "        if last_entry is None or not last_entry:\n",
    "            print(\"Load historic data for \"+station + \" ...\")\n",
    "        \n",
    "            file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_2007-2018.csv\")\n",
    "            if os.path.isfile(file_name):\n",
    "                print(\"\\tLoad \"+ file_name)\n",
    "                for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                    chunk = __define_types(chunk, '%Y-%m-%dT%H:%M:%S')\n",
    "                    print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                    __add_data_to_db(config, chunk, station)\n",
    "                    \n",
    "                    if config.historic_data_sleep_sec > 0:\n",
    "                        sleep(config.historic_data_sleep_sec)\n",
    "            else:\n",
    "                print(file_name +\" does not seem to exist.\")\n",
    "                \n",
    "            current_time = datetime.datetime.now(tzlocal.get_localzone())\n",
    "            running_year = 2019\n",
    "            while running_year <= current_time.year:\n",
    "                file_name = os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(running_year) +\".csv\")\n",
    "                if os.path.isfile(file_name):\n",
    "                    print(\"\\tLoad \"+ file_name)\n",
    "                    for chunk in pd.read_csv(file_name, delimiter=',', chunksize=config.historic_data_chunksize):\n",
    "                        chunk = __define_types(chunk, '%Y-%m-%d %H:%M:%S')\n",
    "                        print(\"Add \"+ station +\" from \"+ str(chunk.index[0]) +\" to \"+ str(chunk.index[-1]))\n",
    "                        __add_data_to_db(config, chunk, station)\n",
    "                        \n",
    "                        if config.historic_data_sleep_sec > 0:\n",
    "                            sleep(config.historic_data_sleep_sec)\n",
    "                else:\n",
    "                    print(file_name +\" does not seem to exist.\")\n",
    "                running_year+=1\n",
    "        else:\n",
    "            print(\"There is already data for \"+station + \". No historic data will be imported.\")\n",
    "    \n",
    "        print(\"Historic data for \"+station+\" loaded.\")\n",
    "        \n",
    "        \n",
    "def import_latest_data(config, append_to_csv=False, periodic_read=False):\n",
    "    \"\"\"Reads the latest data from the Wasserschutzpolizei Zurich weather API\n",
    "\n",
    "    Parameters:\n",
    "    config (Config): The Config containing the DB connection info and CSV folder info\n",
    "    append_to_csv (bool): Defines if the data should be appended to a CSV file\n",
    "    periodic_read (bool): Defines if the function should keep reading after it imported the latest data (blocking through a sleep)\n",
    "\n",
    "   \"\"\"\n",
    "    stations = config.get_stations_recorded()\n",
    "    \n",
    "    # access API for current data\n",
    "    current_time = datetime.datetime.now(pytz.utc)\n",
    "    current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    last_db_days = [current_day] * len(stations)\n",
    "    new_data_received = True\n",
    "\n",
    "    for idx, station in enumerate(stations):\n",
    "        last_db_entry = __get_last_db_entry(config, station)\n",
    "        last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "\n",
    "    if periodic_read:\n",
    "        signal.signal(signal.SIGINT, __signal_handler)\n",
    "        print(\"\\nPress Ctrl+C to stop!\\n\")\n",
    "\n",
    "    while True:\n",
    "        current_time = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "        # check if all historic data (retrieved from API) has been processed \n",
    "        last_db_day = max(last_db_days)\n",
    "        if periodic_read and last_db_day >= current_day: \n",
    "            # once every 10 Min\n",
    "            sleep_until = current_time + datetime.timedelta(minutes=10)       \n",
    "            # once per day\n",
    "            # sleep_until = current_time + datetime.timedelta(days=1)\n",
    "            # sleep_until = sleep_until.replace(hour=6, minute=0, second=0, microsecond=0)\n",
    "            sleep_sec = (sleep_until - current_time).total_seconds()\n",
    "\n",
    "            print(\"Sleep for \"+str(sleep_sec) + \"s (from \" + str(current_time) +\" until \"+str(sleep_until) + \") when next data will be queried.\")\n",
    "            sleep(sleep_sec)\n",
    "            current_time = datetime.datetime.now(pytz.utc)\n",
    "            current_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        elif not periodic_read and not new_data_received:\n",
    "            # stop here\n",
    "            return;\n",
    "\n",
    "        new_data_received = False\n",
    "        for idx, station in enumerate(stations):\n",
    "            last_db_entry = __get_last_db_entry(config, station)\n",
    "            last_db_days[idx] = __extract_last_db_day(last_db_entry, station, last_db_days[idx])\n",
    "            data_of_last_db_day = __get_data_of_day(last_db_days[idx], station)\n",
    "\n",
    "            normalized_data = __clean_data(config, data_of_last_db_day, last_db_entry, '%d.%m.%Y %H:%M:%S', station)\n",
    "\n",
    "            if normalized_data.size > 0:\n",
    "                new_data_received = True\n",
    "                __add_data_to_db(config, normalized_data, station)\n",
    "                if append_to_csv:\n",
    "                    __append_df_to_csv(normalized_data, os.path.join(config.historic_data_folder ,\"messwerte_\" + station + \"_\"+ str(current_time.year) +\".csv\"))\n",
    "                print(\"Handle \"+ station +\" from \"+ str(normalized_data.index[0]) +\" to \"+ str(normalized_data.index[-1])) \n",
    "            else:\n",
    "                print(\"No new data received for \"+station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: ['data_import.ipynb', '.ipynb_checkpoints', 'Readme.md']\n",
      "Load historic data for mythenquai ...\n",
      "../data/messwerte_mythenquai_2007-2018.csv does not seem to exist.\n",
      "../data/messwerte_mythenquai_2019.csv does not seem to exist.\n",
      "Historic data for mythenquai loaded.\n",
      "Load historic data for tiefenbrunnen ...\n",
      "../data/messwerte_tiefenbrunnen_2007-2018.csv does not seem to exist.\n",
      "../data/messwerte_tiefenbrunnen_2019.csv does not seem to exist.\n",
      "Historic data for tiefenbrunnen loaded.\n",
      "\n",
      "Press Ctrl+C to stop!\n",
      "\n",
      "Sleep for 600.0s (from 2019-11-04 15:39:52.170230+00:00 until 2019-11-04 15:49:52.170230+00:00) when next data will be queried.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Start load data code block\n",
    "print(\"Working dir: \"+str(os.listdir(os.getcwd())))\n",
    "\n",
    "config = Config()\n",
    "# define CSV path\n",
    "config.historic_data_folder='..'+os.sep+'data'\n",
    "\n",
    "# connect to DB\n",
    "connect_db(config)\n",
    "# clean DB\n",
    "clean_db(config)\n",
    "#client.get_list_database()\n",
    "import_historic_data(config)\n",
    "import_latest_data(config, append_to_csv=True, periodic_read=True)\n",
    "\n",
    "# End load data code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
